{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a0a0e37",
   "metadata": {},
   "source": [
    "### ✅ SECTION 0: Setup & Imports + Globals\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4599798d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ All libraries loaded and configurations set.\n"
     ]
    }
   ],
   "source": [
    "# 📦 Standard & Typing\n",
    "import os\n",
    "import time\n",
    "from typing import List, Dict, Union, Any\n",
    "\n",
    "# 📁 File Parsing\n",
    "import fitz  # PyMuPDF for PDFs\n",
    "import docx2txt\n",
    "import pandas as pd\n",
    "import langdetect\n",
    "\n",
    "# 🤖 LLM & Embedding\n",
    "from llama_cpp import Llama\n",
    "import tiktoken\n",
    "from tiktoken import get_encoding\n",
    "from chromadb import PersistentClient\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# 📊 Evaluation\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "# 🌍 Translation (fallback if needed)\n",
    "from transformers import pipeline\n",
    "\n",
    "# 🧠 Memory\n",
    "previous_qas = []\n",
    "\n",
    "# ⚙️ Global Configuration\n",
    "USE_GPU = True\n",
    "GPU_LAYERS = 20  \n",
    "ENCODER = get_encoding(\"cl100k_base\")\n",
    "\n",
    "# ⚡ Performance Logger\n",
    "def log_tokens(text: str, seconds: float, label=\"LLM\"):\n",
    "    tokens = len(ENCODER.encode(text))\n",
    "    tps = tokens / seconds if seconds > 0 else 0\n",
    "    print(f\"🔢 {label}: {tokens} tokens in {seconds:.2f}s → {tps:.2f} tokens/sec\")\n",
    "\n",
    "print(\"✅ All libraries loaded and configurations set.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b72ee98",
   "metadata": {},
   "source": [
    "### ✅ SECTION 1: Document Extraction & Chunking\n",
    "\n",
    "#### Ensures proper text extraction from .pdf, .docx, .csv, .xls[x/m].\n",
    "\n",
    "All extracted outputs are standardized into a consistent format.\n",
    "\n",
    "Table content is flattened but readable.\n",
    "\n",
    "Uses smarter chunking with overlap for better LLM context."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b35156f",
   "metadata": {},
   "source": [
    "📥 1A. Document Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "af02d4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text(file_path: str) -> List[Dict[str, Union[str, int]]]:\n",
    "    ext = file_path.lower().split('.')[-1]\n",
    "    filename = os.path.basename(file_path)\n",
    "\n",
    "    if ext == 'pdf':\n",
    "        doc = fitz.open(file_path)\n",
    "        return [\n",
    "            {\n",
    "                \"source\": filename,\n",
    "                \"page\": i + 1,\n",
    "                \"text\": page.get_text()\n",
    "            } for i, page in enumerate(doc)\n",
    "        ]\n",
    "\n",
    "    elif ext == 'docx':\n",
    "        text = docx2txt.process(file_path)\n",
    "        return [{\n",
    "            \"source\": filename,\n",
    "            \"page\": 1,\n",
    "            \"text\": text\n",
    "        }]\n",
    "\n",
    "    elif ext in ['xls', 'xlsx', 'xlsm']:\n",
    "        xls = pd.ExcelFile(file_path)\n",
    "        results = []\n",
    "        for sheet in xls.sheet_names:\n",
    "            df = xls.parse(sheet).astype(str)\n",
    "            results.append({\n",
    "                \"source\": filename,\n",
    "                \"page\": sheet,\n",
    "                \"text\": df.to_string(index=False)\n",
    "            })\n",
    "        return results\n",
    "\n",
    "    elif ext == 'csv':\n",
    "        df = pd.read_csv(file_path).astype(str)\n",
    "        return [{\n",
    "            \"source\": filename,\n",
    "            \"page\": 1,\n",
    "            \"text\": df.to_string(index=False)\n",
    "        }]\n",
    "\n",
    "    else:\n",
    "        return [{\n",
    "            \"source\": filename,\n",
    "            \"page\": 1,\n",
    "            \"text\": f\"[UNSUPPORTED FILE TYPE: {ext}]\"\n",
    "        }]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f2d65",
   "metadata": {},
   "source": [
    "🧩 1B. Smarter Chunking with Overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "757ee045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text: str, max_tokens: int = 500, overlap: int = 50) -> List[str]:\n",
    "    tokens = ENCODER.encode(text)\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(tokens):\n",
    "        end = min(start + max_tokens, len(tokens))\n",
    "        chunk = ENCODER.decode(tokens[start:end])\n",
    "        chunks.append(chunk)\n",
    "        start += max_tokens - overlap\n",
    "    return chunks\n",
    "\n",
    "\n",
    "def detect_table_like(text: str) -> bool:\n",
    "    \"\"\"Heuristic: if lots of tabs or linebreaks, likely a table.\"\"\"\n",
    "    return text.count('\\t') > 5 or text.count('\\n') > 10\n",
    "\n",
    "def chunk_documents(docs: List[Dict[str, Union[str, int]]]) -> List[Dict[str, Any]]:\n",
    "    all_chunks = []\n",
    "    for doc in docs:\n",
    "        file_name = doc[\"source\"]\n",
    "        page = doc[\"page\"]\n",
    "        chunk_type = \"table\" if detect_table_like(doc[\"text\"]) else \"text\"\n",
    "        split_chunks = chunk_text(doc[\"text\"])\n",
    "        for i, chunk in enumerate(split_chunks):\n",
    "            all_chunks.append({\n",
    "                \"source\": file_name,\n",
    "                \"page\": page,\n",
    "                \"chunk_number\": i + 1,\n",
    "                \"type\": chunk_type,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "    return all_chunks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4238dba3",
   "metadata": {},
   "source": [
    "🗂️ 1C. Load All Files and Chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "ef58df73",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\UTAS Ibri\\-- OSOS Test\\OSOS_AI_Tech_Test\\Test2\\venv\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "\n",
      "e:\\UTAS Ibri\\-- OSOS Test\\OSOS_AI_Tech_Test\\Test2\\venv\\lib\\site-packages\\openpyxl\\worksheet\\header_footer.py:48: UserWarning: Cannot parse header or footer so it will be ignored\n",
      "  warn(\"\"\"Cannot parse header or footer so it will be ignored\"\"\")\n",
      "\n",
      "e:\\UTAS Ibri\\-- OSOS Test\\OSOS_AI_Tech_Test\\Test2\\venv\\lib\\site-packages\\openpyxl\\worksheet\\_reader.py:329: UserWarning: Unknown extension is not supported and will be removed\n",
      "  warn(msg)\n",
      "\n",
      "e:\\UTAS Ibri\\-- OSOS Test\\OSOS_AI_Tech_Test\\Test2\\venv\\lib\\site-packages\\openpyxl\\reader\\workbook.py:118: UserWarning: Print area cannot be set to Defined name: 'Wedding budget'!$A:$K.\n",
      "  warn(f\"Print area cannot be set to Defined name: {defn.value}.\")\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📄 Loaded and extracted 220 document entries.\n",
      "🧩 Split into 508 chunks.\n"
     ]
    }
   ],
   "source": [
    "def load_and_chunk_all_documents(folder: str = \"Dr.X Files\") -> List[Dict[str, Any]]:\n",
    "    all_docs = []\n",
    "    for file in os.listdir(folder):\n",
    "        path = os.path.join(folder, file)\n",
    "        if os.path.isfile(path):\n",
    "            extracted = extract_text(path)\n",
    "            all_docs.extend(extracted)\n",
    "\n",
    "    print(f\"📄 Loaded and extracted {len(all_docs)} document entries.\")\n",
    "    chunks = chunk_documents(all_docs)\n",
    "    print(f\"🧩 Split into {len(chunks)} chunks.\")\n",
    "    return chunks\n",
    "\n",
    "# Load and prepare chunks\n",
    "chunks = load_and_chunk_all_documents(\"Dr.X Files\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6227802c",
   "metadata": {},
   "source": [
    "### ✅ SECTION 2: Embedding + Vector DB Storage (ChromaDB + nomic)\n",
    "\n",
    "Connects to a persistent local ChromaDB client.\n",
    "\n",
    "embedding model via embedding_functions.\n",
    "\n",
    "Stores text chunks along with metadata (source, page, chunk_number).\n",
    "\n",
    "Includes performance logging (tokens/sec)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a3760d2",
   "metadata": {},
   "source": [
    "#### 🧠 2A. Setup Vector Store with nomic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f3b793a",
   "metadata": {},
   "source": [
    "🧠 1. Custom Local Embedding Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "bed358f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# class LocalEmbeddingFunction:\n",
    "\n",
    "class LocalEmbeddingFunction:\n",
    "    def __init__(self):\n",
    "       \n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.device = device\n",
    "        # self.model = SentenceTransformer(\"all-MiniLM-L6-v2\", device=device)\n",
    "        self.model = SentenceTransformer(\"nomic-ai/nomic-embed-text-v1.5\", trust_remote_code=True, device=device)\n",
    "\n",
    "        print(f\"🟢 SentenceTransformer loaded on {self.device}\")\n",
    "\n",
    "    def __call__(self, input: list[str]) -> list[list[float]]:\n",
    "        return self.model.encode(input, convert_to_numpy=True, device=\"cuda\" if torch.cuda.is_available() else \"cpu\").tolist()\n",
    "\n",
    "    def embed_query(self, text: str) -> list[float]:\n",
    "        return self.__call__([text])[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d555d5be",
   "metadata": {},
   "source": [
    "🧠 2. Use It in ChromaDB Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "11a23cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:transformers_modules.nomic-ai.nomic-bert-2048.e5042dce39060cc34bc223455f25cf1d26db4655.modeling_hf_nomic_bert:<All keys matched successfully>\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🟢 SentenceTransformer loaded on cuda\n"
     ]
    }
   ],
   "source": [
    "# 🌐 Initialize ChromaDB  localy embedding\n",
    "embedding_fn = LocalEmbeddingFunction()\n",
    "\n",
    "client = PersistentClient(path=\"vector_store\")\n",
    "collection = client.get_or_create_collection(\n",
    "    name=\"dr_x_chunks\",\n",
    "    embedding_function=embedding_fn\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12cde65e",
   "metadata": {},
   "source": [
    "🧠 2B. Index Chunks with Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "33b15164",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Embedding: 500 tokens in 0.03s → 16129.46 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21590.08 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21776.15 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22809.51 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21920.23 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 14350.29 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22074.59 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21436.92 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21053.63 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21146.19 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 16883.92 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19183.96 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20798.68 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20601.52 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19570.84 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18218.20 tokens/sec\n",
      "🔢 Embedding: 220 tokens in 0.02s → 9072.51 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22677.31 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20136.07 tokens/sec\n",
      "🔢 Embedding: 363 tokens in 0.02s → 16195.09 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20726.74 tokens/sec\n",
      "🔢 Embedding: 499 tokens in 0.03s → 19874.44 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 17585.74 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21627.93 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 17969.84 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20232.23 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21297.37 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.04s → 11780.89 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20229.50 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21004.71 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22383.71 tokens/sec\n",
      "🔢 Embedding: 63 tokens in 0.02s → 2823.45 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22119.99 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21196.84 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21680.02 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 23279.19 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21505.28 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21369.42 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19757.24 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21717.29 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21740.03 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20832.98 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20833.60 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21652.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21928.48 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20775.61 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20440.08 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19877.65 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19189.40 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18845.21 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20924.65 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20700.75 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21943.85 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21885.91 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21369.42 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20889.42 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21680.92 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21973.74 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 17457.21 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19956.15 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18361.60 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20598.89 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20355.96 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21412.40 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19062.42 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21533.55 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20737.39 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18824.58 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.04s → 13403.59 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21309.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22440.47 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20834.43 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21028.93 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21042.43 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22487.88 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21307.54 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21037.58 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20868.01 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21085.59 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21296.50 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20000.69 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21117.02 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21804.90 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21687.87 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18671.06 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22057.87 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18897.69 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21245.59 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21491.62 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18883.39 tokens/sec\n",
      "🔢 Embedding: 463 tokens in 0.02s → 19973.49 tokens/sec\n",
      "🔢 Embedding: 13 tokens in 0.02s → 609.11 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20942.83 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19940.59 tokens/sec\n",
      "🔢 Embedding: 194 tokens in 0.02s → 8551.26 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20619.14 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21378.57 tokens/sec\n",
      "🔢 Embedding: 287 tokens in 0.02s → 12424.55 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21781.12 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21594.08 tokens/sec\n",
      "🔢 Embedding: 349 tokens in 0.03s → 13936.95 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20762.44 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18267.56 tokens/sec\n",
      "🔢 Embedding: 367 tokens in 0.02s → 16609.94 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19649.87 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19109.84 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22480.65 tokens/sec\n",
      "🔢 Embedding: 58 tokens in 0.02s → 2454.67 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21427.72 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.04s → 14177.51 tokens/sec\n",
      "🔢 Embedding: 231 tokens in 0.04s → 6153.33 tokens/sec\n",
      "🔢 Embedding: 458 tokens in 0.04s → 12717.59 tokens/sec\n",
      "🔢 Embedding: 8 tokens in 0.02s → 323.42 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18112.94 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22404.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22051.14 tokens/sec\n",
      "🔢 Embedding: 332 tokens in 0.02s → 14716.39 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22640.58 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 23166.29 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22608.85 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22982.99 tokens/sec\n",
      "🔢 Embedding: 232 tokens in 0.02s → 10793.99 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21313.39 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22510.81 tokens/sec\n",
      "🔢 Embedding: 65 tokens in 0.04s → 1755.68 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22233.73 tokens/sec\n",
      "🔢 Embedding: 299 tokens in 0.02s → 13688.92 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 16036.09 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21345.49 tokens/sec\n",
      "🔢 Embedding: 243 tokens in 0.03s → 9709.04 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22719.07 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19726.58 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20986.84 tokens/sec\n",
      "🔢 Embedding: 153 tokens in 0.02s → 6899.71 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21724.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22200.31 tokens/sec\n",
      "🔢 Embedding: 285 tokens in 0.02s → 12884.68 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18493.73 tokens/sec\n",
      "🔢 Embedding: 281 tokens in 0.02s → 12494.83 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20709.75 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21711.45 tokens/sec\n",
      "🔢 Embedding: 263 tokens in 0.04s → 7049.93 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21602.53 tokens/sec\n",
      "🔢 Embedding: 418 tokens in 0.02s → 18555.53 tokens/sec\n",
      "🔢 Embedding: 318 tokens in 0.02s → 14241.81 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20783.84 tokens/sec\n",
      "🔢 Embedding: 419 tokens in 0.02s → 18153.60 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22546.38 tokens/sec\n",
      "🔢 Embedding: 419 tokens in 0.02s → 18727.36 tokens/sec\n",
      "🔢 Embedding: 290 tokens in 0.02s → 13133.10 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 23278.41 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20727.35 tokens/sec\n",
      "🔢 Embedding: 190 tokens in 0.02s → 8431.83 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21906.03 tokens/sec\n",
      "🔢 Embedding: 347 tokens in 0.02s → 15576.02 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19634.97 tokens/sec\n",
      "🔢 Embedding: 355 tokens in 0.02s → 15232.35 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20877.16 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21528.46 tokens/sec\n",
      "🔢 Embedding: 101 tokens in 0.02s → 4530.31 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22101.34 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22758.02 tokens/sec\n",
      "🔢 Embedding: 163 tokens in 0.02s → 7235.39 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18742.81 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21136.38 tokens/sec\n",
      "🔢 Embedding: 293 tokens in 0.02s → 12651.26 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22223.60 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20793.32 tokens/sec\n",
      "🔢 Embedding: 137 tokens in 0.02s → 6113.30 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22022.20 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 16110.50 tokens/sec\n",
      "🔢 Embedding: 269 tokens in 0.02s → 11902.31 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22004.18 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22594.73 tokens/sec\n",
      "🔢 Embedding: 323 tokens in 0.02s → 14942.15 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22803.56 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22578.18 tokens/sec\n",
      "🔢 Embedding: 158 tokens in 0.02s → 7159.37 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22931.72 tokens/sec\n",
      "🔢 Embedding: 352 tokens in 0.03s → 11875.19 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22888.43 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22279.79 tokens/sec\n",
      "🔢 Embedding: 274 tokens in 0.02s → 12083.01 tokens/sec\n",
      "🔢 Embedding: 361 tokens in 0.02s → 16390.38 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22938.25 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21529.57 tokens/sec\n",
      "🔢 Embedding: 212 tokens in 0.02s → 9131.07 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21377.04 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19373.05 tokens/sec\n",
      "🔢 Embedding: 260 tokens in 0.02s → 11438.81 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22725.23 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21098.53 tokens/sec\n",
      "🔢 Embedding: 206 tokens in 0.02s → 9102.87 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21669.05 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21580.74 tokens/sec\n",
      "🔢 Embedding: 164 tokens in 0.02s → 7194.35 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22063.67 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22631.30 tokens/sec\n",
      "🔢 Embedding: 175 tokens in 0.02s → 8042.02 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22668.24 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22361.75 tokens/sec\n",
      "🔢 Embedding: 196 tokens in 0.02s → 8899.42 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21881.80 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 23086.22 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19744.41 tokens/sec\n",
      "🔢 Embedding: 173 tokens in 0.02s → 7701.28 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22154.11 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20715.89 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21094.71 tokens/sec\n",
      "🔢 Embedding: 143 tokens in 0.02s → 6626.80 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19985.44 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22714.64 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22679.02 tokens/sec\n",
      "🔢 Embedding: 163 tokens in 0.03s → 5957.56 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18372.22 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21413.49 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21385.98 tokens/sec\n",
      "🔢 Embedding: 110 tokens in 0.02s → 5336.88 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 23395.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22024.05 tokens/sec\n",
      "🔢 Embedding: 468 tokens in 0.02s → 21521.75 tokens/sec\n",
      "🔢 Embedding: 18 tokens in 0.02s → 877.87 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22203.36 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22621.05 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21595.41 tokens/sec\n",
      "🔢 Embedding: 160 tokens in 0.02s → 7093.89 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22649.14 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 16407.71 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20326.76 tokens/sec\n",
      "🔢 Embedding: 81 tokens in 0.02s → 3463.86 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21826.01 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21492.06 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22582.32 tokens/sec\n",
      "🔢 Embedding: 58 tokens in 0.02s → 2651.32 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22209.24 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22349.36 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22758.27 tokens/sec\n",
      "🔢 Embedding: 220 tokens in 0.02s → 9897.21 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20551.65 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22110.43 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22359.13 tokens/sec\n",
      "🔢 Embedding: 96 tokens in 0.02s → 4068.07 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21213.57 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22720.06 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22151.06 tokens/sec\n",
      "🔢 Embedding: 176 tokens in 0.02s → 8218.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 23405.19 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 23009.47 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22677.55 tokens/sec\n",
      "🔢 Embedding: 242 tokens in 0.02s → 10653.93 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21239.13 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22496.80 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21935.36 tokens/sec\n",
      "🔢 Embedding: 253 tokens in 0.03s → 9119.93 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19731.22 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19883.31 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21432.75 tokens/sec\n",
      "🔢 Embedding: 55 tokens in 0.03s → 2141.04 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21636.85 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20709.54 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20729.61 tokens/sec\n",
      "🔢 Embedding: 167 tokens in 0.02s → 7270.06 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.04s → 13610.00 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22331.51 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22052.30 tokens/sec\n",
      "🔢 Embedding: 111 tokens in 0.02s → 5012.68 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19873.89 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 16992.82 tokens/sec\n",
      "🔢 Embedding: 377 tokens in 0.03s → 14658.74 tokens/sec\n",
      "🔢 Embedding: 431 tokens in 0.02s → 19013.68 tokens/sec\n",
      "🔢 Embedding: 323 tokens in 0.02s → 14690.05 tokens/sec\n",
      "🔢 Embedding: 337 tokens in 0.02s → 15315.64 tokens/sec\n",
      "🔢 Embedding: 211 tokens in 0.02s → 9546.80 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21579.86 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21601.42 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21970.75 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22503.80 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22100.87 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20713.23 tokens/sec\n",
      "🔢 Embedding: 487 tokens in 0.02s → 21469.01 tokens/sec\n",
      "🔢 Embedding: 37 tokens in 0.02s → 1664.19 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18619.01 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21158.99 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21789.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22021.04 tokens/sec\n",
      "🔢 Embedding: 138 tokens in 0.02s → 6715.87 tokens/sec\n",
      "🔢 Embedding: 67 tokens in 0.02s → 3061.44 tokens/sec\n",
      "🔢 Embedding: 169 tokens in 0.02s → 7588.45 tokens/sec\n",
      "🔢 Embedding: 341 tokens in 0.02s → 15156.81 tokens/sec\n",
      "🔢 Embedding: 317 tokens in 0.02s → 13990.72 tokens/sec\n",
      "🔢 Embedding: 220 tokens in 0.02s → 9155.60 tokens/sec\n",
      "🔢 Embedding: 444 tokens in 0.02s → 19626.61 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20746.63 tokens/sec\n",
      "🔢 Embedding: 90 tokens in 0.02s → 4203.27 tokens/sec\n",
      "🔢 Embedding: 147 tokens in 0.02s → 6826.27 tokens/sec\n",
      "🔢 Embedding: 362 tokens in 0.02s → 16054.33 tokens/sec\n",
      "🔢 Embedding: 390 tokens in 0.02s → 16400.26 tokens/sec\n",
      "🔢 Embedding: 322 tokens in 0.02s → 13320.77 tokens/sec\n",
      "🔢 Embedding: 452 tokens in 0.03s → 17900.34 tokens/sec\n",
      "🔢 Embedding: 2 tokens in 0.03s → 74.95 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20772.52 tokens/sec\n",
      "🔢 Embedding: 105 tokens in 0.03s → 3687.66 tokens/sec\n",
      "🔢 Embedding: 376 tokens in 0.02s → 15265.01 tokens/sec\n",
      "🔢 Embedding: 414 tokens in 0.02s → 16582.23 tokens/sec\n",
      "🔢 Embedding: 477 tokens in 0.02s → 20004.43 tokens/sec\n",
      "🔢 Embedding: 27 tokens in 0.02s → 1166.22 tokens/sec\n",
      "🔢 Embedding: 441 tokens in 0.02s → 17867.41 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18892.24 tokens/sec\n",
      "🔢 Embedding: 61 tokens in 0.03s → 2369.40 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22837.83 tokens/sec\n",
      "🔢 Embedding: 60 tokens in 0.03s → 2298.79 tokens/sec\n",
      "🔢 Embedding: 417 tokens in 0.02s → 17713.80 tokens/sec\n",
      "🔢 Embedding: 487 tokens in 0.02s → 21043.68 tokens/sec\n",
      "🔢 Embedding: 37 tokens in 0.03s → 1437.51 tokens/sec\n",
      "🔢 Embedding: 466 tokens in 0.03s → 16558.33 tokens/sec\n",
      "🔢 Embedding: 16 tokens in 0.03s → 623.17 tokens/sec\n",
      "🔢 Embedding: 482 tokens in 0.03s → 17951.75 tokens/sec\n",
      "🔢 Embedding: 32 tokens in 0.02s → 1347.18 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20594.84 tokens/sec\n",
      "🔢 Embedding: 87 tokens in 0.02s → 4490.74 tokens/sec\n",
      "🔢 Embedding: 476 tokens in 0.03s → 17496.02 tokens/sec\n",
      "🔢 Embedding: 26 tokens in 0.02s → 1151.78 tokens/sec\n",
      "🔢 Embedding: 415 tokens in 0.03s → 14174.68 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 17781.21 tokens/sec\n",
      "🔢 Embedding: 78 tokens in 0.03s → 3069.78 tokens/sec\n",
      "🔢 Embedding: 392 tokens in 0.02s → 16285.49 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20517.47 tokens/sec\n",
      "🔢 Embedding: 53 tokens in 0.03s → 1683.15 tokens/sec\n",
      "🔢 Embedding: 398 tokens in 0.02s → 15920.24 tokens/sec\n",
      "🔢 Embedding: 486 tokens in 0.03s → 18338.31 tokens/sec\n",
      "🔢 Embedding: 36 tokens in 0.02s → 1473.55 tokens/sec\n",
      "🔢 Embedding: 491 tokens in 0.02s → 20273.91 tokens/sec\n",
      "🔢 Embedding: 41 tokens in 0.02s → 1672.26 tokens/sec\n",
      "🔢 Embedding: 495 tokens in 0.02s → 21622.15 tokens/sec\n",
      "🔢 Embedding: 45 tokens in 0.02s → 1952.11 tokens/sec\n",
      "🔢 Embedding: 435 tokens in 0.02s → 17746.54 tokens/sec\n",
      "🔢 Embedding: 404 tokens in 0.02s → 16252.47 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 15734.34 tokens/sec\n",
      "🔢 Embedding: 68 tokens in 0.03s → 2485.51 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18535.25 tokens/sec\n",
      "🔢 Embedding: 57 tokens in 0.03s → 2272.21 tokens/sec\n",
      "🔢 Embedding: 461 tokens in 0.03s → 17810.45 tokens/sec\n",
      "🔢 Embedding: 11 tokens in 0.02s → 458.64 tokens/sec\n",
      "🔢 Embedding: 351 tokens in 0.02s → 15180.46 tokens/sec\n",
      "🔢 Embedding: 484 tokens in 0.03s → 15232.56 tokens/sec\n",
      "🔢 Embedding: 34 tokens in 0.02s → 1479.30 tokens/sec\n",
      "🔢 Embedding: 388 tokens in 0.03s → 14825.05 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18532.63 tokens/sec\n",
      "🔢 Embedding: 70 tokens in 0.02s → 2906.77 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20664.04 tokens/sec\n",
      "🔢 Embedding: 62 tokens in 0.03s → 2428.98 tokens/sec\n",
      "🔢 Embedding: 448 tokens in 0.03s → 17749.81 tokens/sec\n",
      "🔢 Embedding: 394 tokens in 0.02s → 16452.02 tokens/sec\n",
      "🔢 Embedding: 441 tokens in 0.02s → 18375.05 tokens/sec\n",
      "🔢 Embedding: 442 tokens in 0.03s → 17575.84 tokens/sec\n",
      "🔢 Embedding: 390 tokens in 0.02s → 16108.27 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19397.06 tokens/sec\n",
      "🔢 Embedding: 82 tokens in 0.02s → 3748.14 tokens/sec\n",
      "🔢 Embedding: 483 tokens in 0.03s → 19141.94 tokens/sec\n",
      "🔢 Embedding: 33 tokens in 0.02s → 1482.63 tokens/sec\n",
      "🔢 Embedding: 398 tokens in 0.02s → 16126.64 tokens/sec\n",
      "🔢 Embedding: 438 tokens in 0.02s → 18631.52 tokens/sec\n",
      "🔢 Embedding: 492 tokens in 0.02s → 20578.56 tokens/sec\n",
      "🔢 Embedding: 42 tokens in 0.02s → 1847.42 tokens/sec\n",
      "🔢 Embedding: 456 tokens in 0.02s → 19210.55 tokens/sec\n",
      "🔢 Embedding: 6 tokens in 0.02s → 281.44 tokens/sec\n",
      "🔢 Embedding: 397 tokens in 0.02s → 16976.31 tokens/sec\n",
      "🔢 Embedding: 496 tokens in 0.03s → 19655.10 tokens/sec\n",
      "🔢 Embedding: 46 tokens in 0.02s → 1962.83 tokens/sec\n",
      "🔢 Embedding: 454 tokens in 0.03s → 16346.87 tokens/sec\n",
      "🔢 Embedding: 4 tokens in 0.02s → 176.98 tokens/sec\n",
      "🔢 Embedding: 490 tokens in 0.03s → 19449.31 tokens/sec\n",
      "🔢 Embedding: 40 tokens in 0.02s → 1765.99 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20424.95 tokens/sec\n",
      "🔢 Embedding: 52 tokens in 0.02s → 2248.10 tokens/sec\n",
      "🔢 Embedding: 430 tokens in 0.03s → 17003.24 tokens/sec\n",
      "🔢 Embedding: 492 tokens in 0.04s → 13307.69 tokens/sec\n",
      "🔢 Embedding: 42 tokens in 0.02s → 1886.82 tokens/sec\n",
      "🔢 Embedding: 396 tokens in 0.03s → 12817.51 tokens/sec\n",
      "🔢 Embedding: 446 tokens in 0.03s → 16267.45 tokens/sec\n",
      "🔢 Embedding: 355 tokens in 0.03s → 12229.60 tokens/sec\n",
      "🔢 Embedding: 446 tokens in 0.03s → 17049.39 tokens/sec\n",
      "🔢 Embedding: 447 tokens in 0.02s → 18616.36 tokens/sec\n",
      "🔢 Embedding: 455 tokens in 0.02s → 19630.80 tokens/sec\n",
      "🔢 Embedding: 5 tokens in 0.02s → 218.46 tokens/sec\n",
      "🔢 Embedding: 426 tokens in 0.03s → 16965.67 tokens/sec\n",
      "🔢 Embedding: 370 tokens in 0.03s → 14738.10 tokens/sec\n",
      "🔢 Embedding: 471 tokens in 0.03s → 18102.25 tokens/sec\n",
      "🔢 Embedding: 21 tokens in 0.02s → 894.93 tokens/sec\n",
      "🔢 Embedding: 412 tokens in 0.02s → 16886.73 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20345.29 tokens/sec\n",
      "🔢 Embedding: 59 tokens in 0.02s → 2429.28 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20453.63 tokens/sec\n",
      "🔢 Embedding: 100 tokens in 0.02s → 4312.86 tokens/sec\n",
      "🔢 Embedding: 438 tokens in 0.03s → 16563.63 tokens/sec\n",
      "🔢 Embedding: 494 tokens in 0.02s → 19760.30 tokens/sec\n",
      "🔢 Embedding: 44 tokens in 0.02s → 1806.56 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21601.42 tokens/sec\n",
      "🔢 Embedding: 78 tokens in 0.02s → 3124.07 tokens/sec\n",
      "🔢 Embedding: 424 tokens in 0.02s → 18311.21 tokens/sec\n",
      "🔢 Embedding: 360 tokens in 0.02s → 14974.61 tokens/sec\n",
      "🔢 Embedding: 494 tokens in 0.03s → 17995.21 tokens/sec\n",
      "🔢 Embedding: 44 tokens in 0.02s → 1870.84 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19829.72 tokens/sec\n",
      "🔢 Embedding: 67 tokens in 0.02s → 2968.96 tokens/sec\n",
      "🔢 Embedding: 331 tokens in 0.02s → 14281.31 tokens/sec\n",
      "🔢 Embedding: 487 tokens in 0.03s → 18933.54 tokens/sec\n",
      "🔢 Embedding: 37 tokens in 0.02s → 1588.69 tokens/sec\n",
      "🔢 Embedding: 431 tokens in 0.03s → 17234.18 tokens/sec\n",
      "🔢 Embedding: 437 tokens in 0.02s → 17974.47 tokens/sec\n",
      "🔢 Embedding: 475 tokens in 0.02s → 19272.50 tokens/sec\n",
      "🔢 Embedding: 25 tokens in 0.03s → 915.93 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19634.42 tokens/sec\n",
      "🔢 Embedding: 63 tokens in 0.03s → 2500.56 tokens/sec\n",
      "🔢 Embedding: 438 tokens in 0.02s → 18251.14 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21474.89 tokens/sec\n",
      "🔢 Embedding: 89 tokens in 0.02s → 3641.67 tokens/sec\n",
      "🔢 Embedding: 422 tokens in 0.02s → 17354.95 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20873.00 tokens/sec\n",
      "🔢 Embedding: 58 tokens in 0.02s → 2452.93 tokens/sec\n",
      "🔢 Embedding: 354 tokens in 0.03s → 12538.60 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22411.22 tokens/sec\n",
      "🔢 Embedding: 58 tokens in 0.02s → 2385.37 tokens/sec\n",
      "🔢 Embedding: 454 tokens in 0.02s → 19507.19 tokens/sec\n",
      "🔢 Embedding: 4 tokens in 0.02s → 170.65 tokens/sec\n",
      "🔢 Embedding: 380 tokens in 0.03s → 11468.26 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19433.91 tokens/sec\n",
      "🔢 Embedding: 103 tokens in 0.07s → 1410.80 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19295.87 tokens/sec\n",
      "🔢 Embedding: 77 tokens in 0.02s → 3237.55 tokens/sec\n",
      "🔢 Embedding: 439 tokens in 0.02s → 18401.22 tokens/sec\n",
      "🔢 Embedding: 419 tokens in 0.02s → 16988.05 tokens/sec\n",
      "🔢 Embedding: 482 tokens in 0.02s → 20038.80 tokens/sec\n",
      "🔢 Embedding: 32 tokens in 0.02s → 1381.10 tokens/sec\n",
      "🔢 Embedding: 455 tokens in 0.03s → 13714.66 tokens/sec\n",
      "🔢 Embedding: 5 tokens in 0.02s → 216.36 tokens/sec\n",
      "🔢 Embedding: 309 tokens in 0.02s → 12549.89 tokens/sec\n",
      "🔢 Embedding: 463 tokens in 0.02s → 19208.15 tokens/sec\n",
      "🔢 Embedding: 13 tokens in 0.03s → 474.22 tokens/sec\n",
      "🔢 Embedding: 464 tokens in 0.03s → 16077.96 tokens/sec\n",
      "🔢 Embedding: 14 tokens in 0.02s → 596.28 tokens/sec\n",
      "🔢 Embedding: 405 tokens in 0.03s → 14843.66 tokens/sec\n",
      "🔢 Embedding: 387 tokens in 0.03s → 13745.88 tokens/sec\n",
      "🔢 Embedding: 487 tokens in 0.02s → 20016.13 tokens/sec\n",
      "🔢 Embedding: 37 tokens in 0.02s → 1530.63 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20824.71 tokens/sec\n",
      "🔢 Embedding: 71 tokens in 0.02s → 3068.54 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20256.86 tokens/sec\n",
      "🔢 Embedding: 60 tokens in 0.02s → 2523.55 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 22179.41 tokens/sec\n",
      "🔢 Embedding: 78 tokens in 0.03s → 3045.52 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20392.18 tokens/sec\n",
      "🔢 Embedding: 88 tokens in 0.02s → 3776.49 tokens/sec\n",
      "🔢 Embedding: 295 tokens in 0.02s → 12033.49 tokens/sec\n",
      "🔢 Embedding: 379 tokens in 0.03s → 11293.43 tokens/sec\n",
      "🔢 Embedding: 454 tokens in 0.03s → 17792.07 tokens/sec\n",
      "🔢 Embedding: 4 tokens in 0.02s → 172.92 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 17420.23 tokens/sec\n",
      "🔢 Embedding: 70 tokens in 0.02s → 3065.79 tokens/sec\n",
      "🔢 Embedding: 470 tokens in 0.03s → 18437.54 tokens/sec\n",
      "🔢 Embedding: 20 tokens in 0.02s → 920.93 tokens/sec\n",
      "🔢 Embedding: 461 tokens in 0.02s → 19369.44 tokens/sec\n",
      "🔢 Embedding: 11 tokens in 0.02s → 464.17 tokens/sec\n",
      "🔢 Embedding: 492 tokens in 0.03s → 18117.31 tokens/sec\n",
      "🔢 Embedding: 42 tokens in 0.02s → 1771.26 tokens/sec\n",
      "🔢 Embedding: 240 tokens in 0.04s → 6402.30 tokens/sec\n",
      "🔢 Embedding: 458 tokens in 0.02s → 18510.40 tokens/sec\n",
      "🔢 Embedding: 8 tokens in 0.02s → 352.17 tokens/sec\n",
      "🔢 Embedding: 294 tokens in 0.03s → 10734.12 tokens/sec\n",
      "🔢 Embedding: 75 tokens in 0.02s → 3169.75 tokens/sec\n",
      "🔢 Embedding: 391 tokens in 0.03s → 14636.60 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 18863.35 tokens/sec\n",
      "🔢 Embedding: 50 tokens in 0.02s → 2023.26 tokens/sec\n",
      "🔢 Embedding: 457 tokens in 0.02s → 18393.07 tokens/sec\n",
      "🔢 Embedding: 7 tokens in 0.02s → 296.09 tokens/sec\n",
      "🔢 Embedding: 291 tokens in 0.02s → 11855.91 tokens/sec\n",
      "🔢 Embedding: 192 tokens in 0.03s → 7358.97 tokens/sec\n",
      "🔢 Embedding: 94 tokens in 0.02s → 5482.83 tokens/sec\n",
      "🔢 Embedding: 3 tokens in 0.03s → 104.53 tokens/sec\n",
      "🔢 Embedding: 424 tokens in 0.03s → 16947.65 tokens/sec\n",
      "🔢 Embedding: 186 tokens in 0.03s → 7418.96 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20435.50 tokens/sec\n",
      "🔢 Embedding: 247 tokens in 0.02s → 10160.38 tokens/sec\n",
      "🔢 Embedding: 399 tokens in 0.03s → 15631.68 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20426.54 tokens/sec\n",
      "🔢 Embedding: 317 tokens in 0.02s → 12945.76 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 21441.96 tokens/sec\n",
      "🔢 Embedding: 384 tokens in 0.02s → 15792.02 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20822.43 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20369.99 tokens/sec\n",
      "🔢 Embedding: 219 tokens in 0.02s → 9850.54 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20195.99 tokens/sec\n",
      "🔢 Embedding: 335 tokens in 0.03s → 10468.81 tokens/sec\n",
      "🔢 Embedding: 457 tokens in 0.02s → 18406.32 tokens/sec\n",
      "🔢 Embedding: 7 tokens in 0.02s → 315.31 tokens/sec\n",
      "🔢 Embedding: 323 tokens in 0.05s → 7142.27 tokens/sec\n",
      "🔢 Embedding: 497 tokens in 0.03s → 18138.52 tokens/sec\n",
      "🔢 Embedding: 47 tokens in 0.02s → 1942.44 tokens/sec\n",
      "🔢 Embedding: 360 tokens in 0.02s → 15273.15 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20879.86 tokens/sec\n",
      "🔢 Embedding: 249 tokens in 0.02s → 10527.19 tokens/sec\n",
      "🔢 Embedding: 358 tokens in 0.03s → 12761.10 tokens/sec\n",
      "🔢 Embedding: 266 tokens in 0.03s → 8728.97 tokens/sec\n",
      "🔢 Embedding: 420 tokens in 0.02s → 17996.34 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.03s → 19821.67 tokens/sec\n",
      "🔢 Embedding: 288 tokens in 0.02s → 11951.36 tokens/sec\n",
      "🔢 Embedding: 500 tokens in 0.02s → 20603.74 tokens/sec\n",
      "🔢 Embedding: 431 tokens in 0.02s → 18131.66 tokens/sec\n",
      "✅ Indexed 508 chunks into ChromaDB.\n"
     ]
    }
   ],
   "source": [
    "from uuid import uuid4\n",
    "\n",
    "def index_chunks_in_vector_db(chunks: List[Dict[str, Any]]):\n",
    "    for chunk in chunks:\n",
    "        uid = str(uuid4())\n",
    "        metadata = {\n",
    "            \"source\": chunk[\"source\"],\n",
    "            \"page\": str(chunk[\"page\"]),\n",
    "            \"chunk_number\": chunk[\"chunk_number\"]\n",
    "        }\n",
    "\n",
    "        # Time embedding performance\n",
    "        start_time = time.time()\n",
    "        collection.add(\n",
    "            documents=[chunk[\"text\"]],\n",
    "            metadatas=[metadata],\n",
    "            ids=[uid]\n",
    "        )\n",
    "        duration = time.time() - start_time\n",
    "        log_tokens(chunk[\"text\"], duration, label=\"Embedding\")\n",
    "\n",
    "    print(f\"✅ Indexed {len(chunks)} chunks into ChromaDB.\")\n",
    "\n",
    "# Index now\n",
    "index_chunks_in_vector_db(chunks)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21569f07",
   "metadata": {},
   "source": [
    "✅ SECTION 3: RAG Q&A System (with LLaMA + Conversational Memory)\n",
    "\n",
    "Uses question embedding to query ChromaDB.\n",
    "\n",
    "Retrieves top-k relevant chunks using cosine similarity.\n",
    "\n",
    "Generates the answer using local LLaMA.\n",
    "\n",
    "Maintains multi-turn memory.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b527a4",
   "metadata": {},
   "source": [
    "🧠 3A. Load LLaMA Model (via llama.cpp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "656d78da",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (3008) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ LLaMA model loaded.\n"
     ]
    }
   ],
   "source": [
    "USE_GPU = True\n",
    "GPU_LAYERS = 40  # You can adjust based on your GPU RAM // 32-40 is typical for 8GB \n",
    "llm = Llama(\n",
    "    model_path=\"models/llama3-8B.gguf\",\n",
    "    # n_ctx=2048,\n",
    "    n_ctx=3008,\n",
    "    n_threads=16,\n",
    "    n_gpu_layers=GPU_LAYERS if USE_GPU else 0,\n",
    "    use_mlock=True,\n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "print(\"✅ LLaMA model loaded.\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182996e7",
   "metadata": {},
   "source": [
    "❓ 3B. Ask Question with Retrieval & Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "88021c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_question(query: str, k: int = 5) -> str:\n",
    "    global previous_qas\n",
    "\n",
    "    # 1️ Embed the query\n",
    "    query_embedding = embedding_fn.embed_query(query)\n",
    "\n",
    "    # 2️ Retrieve top-k documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    # 3️ Prepare prompt with context\n",
    "    context_blocks = []\n",
    "    for i in range(len(results[\"documents\"][0])):\n",
    "        meta = results[\"metadatas\"][0][i]\n",
    "        doc = results[\"documents\"][0][i]\n",
    "        block = f\"Source: {meta['source']} (Page: {meta['page']}, Chunk: {meta['chunk_number']})\\n{doc}\"\n",
    "        context_blocks.append(block)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_blocks)\n",
    "    history_text = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in previous_qas])\n",
    "    prompt = f\"\"\"\n",
    "# You are an intelligent assistant trained to answer questions using only the context provided.\n",
    "You are a helpful AI researcher. Answer the user's question using ONLY the context below.\n",
    "\n",
    "\n",
    "{history_text}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Q: {query}\n",
    "A:\"\"\".strip()\n",
    "\n",
    "    # 4️ Generate Answer\n",
    "    start_time = time.time()\n",
    "    output = llm(prompt, max_tokens=300, stop=[\"Q:\", \"User:\"], echo=False)\n",
    "    response = output['choices'][0]['text'].strip()\n",
    "    duration = time.time() - start_time\n",
    "    log_tokens(prompt, duration, label=\"LLM Generation\")\n",
    "\n",
    "    # 5️ Update memory\n",
    "    previous_qas.append((query, response))\n",
    "\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "fefac74e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 LLM Generation: 2682 tokens in 10.71s → 250.52 tokens/sec\n",
      "\n",
      "🧠 Answer:\n",
      " According to the context, Dr. X was researching new approaches and procedures for cancer treatment. Specifically, they were looking into gene therapy, targeted therapy, immunotherapy, and stem cell therapy as potential cancer treatments. They also analyzed clinical trials to gather information on these treatments and their potential outcomes. Dr. X was interested in developing safe and efficient cancer nanomedicines and investigating alternative treatments such as thermal ablation and magnetic hyperthermia. The research was aimed at improving prognosis and outcomes for cancer patients.\n"
     ]
    }
   ],
   "source": [
    "response = ask_question(\"What was Dr. X researching?\")\n",
    "print(\"\\n🧠 Answer:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "bafd2962",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Reset memory\n",
    "# reset_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff0e7028",
   "metadata": {},
   "source": [
    "### ✅ SECTION 4: Translation (Any Language → English or Arabic)\n",
    "\n",
    "Language auto-detection using langdetect.\n",
    "\n",
    "Translation into English or Arabic using a local LLM, or fallback to transformers pipeline if needed.\n",
    "\n",
    "Option to preserve document format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e79893",
   "metadata": {},
   "source": [
    "🌍 4A. Translation Tool (LLM-powered or Transformers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "362789a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_offline_translator(local_model_path=\"./models/opus-mt-mul-en\", device=-1):\n",
    "    from transformers import pipeline\n",
    "    return pipeline(\"translation\", model=local_model_path, device=device, local_files_only=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1a2c802",
   "metadata": {},
   "source": [
    "🔁 4B. Translate Text to English or Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "08bf4342",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_text(text: str, target_lang: str = \"en\") -> str:\n",
    "    detected = langdetect.detect(text)\n",
    "    if detected == target_lang:\n",
    "        return text\n",
    "\n",
    "    prompt = f\"Translate this text from {detected} to {target_lang}:\\n\\n{text}\\n\\nTranslation:\"\n",
    "    try:\n",
    "        # Try translating with LLaMA (simple prompt)\n",
    "        start_time = time.time()\n",
    "        output = llm(prompt, max_tokens=512, stop=[\"\\n\\n\"], echo=False)\n",
    "        translated = output['choices'][0]['text'].strip()\n",
    "        duration = time.time() - start_time\n",
    "        log_tokens(prompt, duration, label=\"Translation\")\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ LLaMA failed, using transformer fallback: {e}\")\n",
    "        translated = get_offline_translator(text, src_lang=detected, tgt_lang=target_lang)[0]['translation_text']\n",
    "\n",
    "    return translated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b3326fe",
   "metadata": {},
   "source": [
    "📂 4C. Translate a Whole Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "739757c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_document(doc: Dict[str, Any], target_lang: str = \"en\") -> Dict[str, Any]:\n",
    "    translated_text = translate_text(doc[\"text\"], target_lang=target_lang)\n",
    "    return {\n",
    "        \"source\": doc[\"source\"],\n",
    "        \"page\": doc[\"page\"],\n",
    "        \"translated_text\": translated_text\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fefad5ad",
   "metadata": {},
   "source": [
    "🧪 Example: Translate Document to Arabic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4cf813a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Translation: 511 tokens in 0.67s → 766.14 tokens/sec\n",
      "🌐 Translated Output:\n",
      "\n",
      "نقدم هنا تفاصيل 10 دراسة منشورة حول إستخدام الأعشاب الوعرية في إزالة الكربون من التربة.\n"
     ]
    }
   ],
   "source": [
    "sample_doc = chunks[0]  # You can pick any chunk or document\n",
    "translated_doc = translate_document(sample_doc, target_lang=\"ar\")\n",
    "\n",
    "print(\"🌐 Translated Output:\\n\")\n",
    "print(translated_doc[\"translated_text\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f06e291",
   "metadata": {},
   "source": [
    "### ✅ SECTION 5: Summarization + ROUGE Evaluation\n",
    "* LLM-based summarization using your local llama_cpp model.\n",
    "\n",
    "* ROUGE metric to evaluate summary quality.\n",
    "\n",
    "* Modular design: can summarize per document, per chunk, or whole corpus.\n",
    "\n",
    "* Support for different summarization strategies (basic prompt engineering).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cd39f49",
   "metadata": {},
   "source": [
    "🧠 5A. Summarization Function via LLaMA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "02252b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summarize_text(text: str, strategy: str = \"default\") -> str:\n",
    "    if strategy == \"default\":\n",
    "        prompt = f\"Summarize the following scientific text:\\n\\n{text}\\n\\nSummary:\"\n",
    "    elif strategy == \"insight\":\n",
    "        prompt = f\"Extract the key findings and main ideas from this text:\\n\\n{text}\\n\\nKey Findings:\"\n",
    "    else:\n",
    "        prompt = f\"Give a brief overview of the text:\\n\\n{text}\\n\\nOverview:\"\n",
    "\n",
    "    start_time = time.time()\n",
    "    output = llm(prompt, max_tokens=256, stop=[\"\\n\\n\"], echo=False)\n",
    "    summary = output['choices'][0]['text'].strip()\n",
    "    duration = time.time() - start_time\n",
    "    log_tokens(prompt, duration, label=\"Summarization\")\n",
    "    return summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "263d86df",
   "metadata": {},
   "source": [
    "🧪 5B. Run Summarization on a Chunk or Full Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "f11ea03e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Summarization: 516 tokens in 1.10s → 471.22 tokens/sec\n",
      "📝 Summary:\n",
      " The studies in this meta-analysis were conducted in the Mediterranean climate region and were focused on the effects of turfgrass on soil organic carbon (SOC) levels. The findings suggest that the impact of turfgrass on SOC levels is dependent on the type of turfgrass used and the conditions under which it is grown.\n"
     ]
    }
   ],
   "source": [
    "# Example: summarizing one of the document chunks\n",
    "sample_text = chunks[0]['text']\n",
    "summary = summarize_text(sample_text, strategy=\"insight\")\n",
    "\n",
    "print(\"📝 Summary:\\n\", summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f0affd",
   "metadata": {},
   "source": [
    "📊 5C. Evaluate with ROUGE Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "fa1a31ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(reference: str, generated: str):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "\n",
    "    print(\"\\n📈 ROUGE Evaluation:\")\n",
    "    for k, v in scores.items():\n",
    "        print(f\"{k}: P={v.precision:.3f}, R={v.recall:.3f}, F1={v.fmeasure:.3f}\")\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c38ce62c",
   "metadata": {},
   "source": [
    "📊 5C. Evaluate with ROUGE Score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "fc1c1524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_summary(reference: str, generated: str):\n",
    "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "    scores = scorer.score(reference, generated)\n",
    "\n",
    "    print(\"\\n📈 ROUGE Evaluation:\")\n",
    "    for k, v in scores.items():\n",
    "        print(f\"{k}: P={v.precision:.3f}, R={v.recall:.3f}, F1={v.fmeasure:.3f}\")\n",
    "\n",
    "    return scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "121e23e1",
   "metadata": {},
   "source": [
    "🧪 5D. Test ROUGE on Manual Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "f4a9d1e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📈 ROUGE Evaluation:\n",
      "rouge1: P=0.167, R=0.333, F1=0.222\n",
      "rouge2: P=0.019, R=0.038, F1=0.025\n",
      "rougeL: P=0.111, R=0.222, F1=0.148\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'rouge1': Score(precision=0.16666666666666666, recall=0.3333333333333333, fmeasure=0.2222222222222222),\n",
       " 'rouge2': Score(precision=0.018867924528301886, recall=0.038461538461538464, fmeasure=0.02531645569620253),\n",
       " 'rougeL': Score(precision=0.1111111111111111, recall=0.2222222222222222, fmeasure=0.14814814814814814)}"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reference summary\n",
    "# reference_summary = \"Dr. X focused on the fields of new cancer treatments, gene therapy, and stem cell therapy in his latest publications.\"\n",
    "reference_summary = \"The SOC stock in the top 10 cm of soil in turfgrass plots was 0.75 to 3.0 times higher than that in non-turf plots.\"\n",
    "\n",
    "\n",
    "evaluate_summary(reference_summary, summary)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3970f899",
   "metadata": {},
   "source": [
    "### ✅ SECTION 6: System Logging, Error Handling \n",
    "\n",
    "Token-per-second tracking \n",
    "\n",
    "Edge case handling: empty retrievals, hallucinations.\n",
    "\n",
    "Safe fallback prompts.\n",
    "\n",
    "Summary of utilities and final save/export options."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63aacb09",
   "metadata": {},
   "source": [
    "🛡️ 6A. Fallback for No Retrievals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "ca210785",
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_ask_question(query: str, k: int = 5) -> str:\n",
    "    global previous_qas\n",
    "\n",
    "    query_embedding = embedding_fn.embed_query(query)\n",
    "\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    if not results[\"documents\"][0]:\n",
    "        return \"❌ Sorry, I couldn't find relevant information in Dr. X's documents.\"\n",
    "\n",
    "    context_blocks = []\n",
    "    for i in range(len(results[\"documents\"][0])):\n",
    "        meta = results[\"metadatas\"][0][i]\n",
    "        doc = results[\"documents\"][0][i]\n",
    "        context_blocks.append(f\"Source: {meta['source']} (Page {meta['page']}, Chunk {meta['chunk_number']})\\n{doc}\")\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_blocks)\n",
    "    history_text = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in previous_qas])\n",
    "    prompt = f\"\"\"\n",
    "You are a careful assistant. Respond using only the context provided.\n",
    "\n",
    "{history_text}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Q: {query}\n",
    "A:\"\"\".strip()\n",
    "\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        output = llm(prompt, max_tokens=300, stop=[\"Q:\", \"User:\"], echo=False)\n",
    "        response = output['choices'][0]['text'].strip()\n",
    "        duration = time.time() - start_time\n",
    "        log_tokens(prompt, duration, label=\"Safe RAG\")\n",
    "    except Exception as e:\n",
    "        response = f\"❗ Generation failed: {e}\"\n",
    "\n",
    "    previous_qas.append((query, response))\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "835c5a04",
   "metadata": {},
   "source": [
    "🧾 6B. Utility: Reset Memory and Cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "0d845ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reset_session():\n",
    "    global previous_qas\n",
    "    previous_qas = []\n",
    "    print(\"🧠 Conversation memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddfd0a96",
   "metadata": {},
   "source": [
    "💾 6C. Optional: Save Summaries or Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "a181c0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_txt(text: str, filename: str):\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)\n",
    "    print(f\"💾 Saved to {filename}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7e46082",
   "metadata": {},
   "source": [
    "#### 🧠 6D. Full Pipeline Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "57643622",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧠 Conversation memory cleared.\n",
      "🔢 Safe RAG: 2657 tokens in 1.59s → 1674.57 tokens/sec\n",
      "🧠 Answer:\n",
      " Dr. X focused on the fields of cancer nanomedicines, gene therapy, stem cell therapy, and targeted therapy in his latest publications. He is particularly interested in exploring the potential benefits and limitations of these new approaches for the treatment of cancer.\n",
      "🔢 Translation: 60 tokens in 1.26s → 47.46 tokens/sec\n",
      "\n",
      "🌍 Arabic Translation:\n",
      " دكتور إكس تتمحور اهتماماته في مجال العلاج النانو لمعظم أنواع السرطان، والtherapy الجينية، والtherapy خلايا الجذع، والtherapy المستهدفة في آخر ما نشره. ويبدو أن دكتور إكس مهتم بشكل خاص في استكشاف المنافع المحتملة ونقاط الضعف لهذه الطرق الجديدة للعلاج للسرطان.\n",
      "🔢 Summarization: 65 tokens in 1.75s → 37.20 tokens/sec\n",
      "\n",
      "📄 Summary:\n",
      " Dr. X's research has shown that cancer nanomedicines can improve the delivery of chemotherapy agents to tumor cells while reducing systemic toxicity. Gene therapy, which involves delivering therapeutic genes directly to cancer cells, can potentially reverse malignant behavior and even cure certain types of cancer. Stem cell therapy, which involves using stem cells to repair damaged tissue and promote healing, may also have benefits in cancer treatment by protecting healthy tissues from the effects of chemotherapy and radiation. Targeted therapy, which involves using drugs or antibodies to target specific molecules that are overexpressed in cancer cells, can be highly effective in treating certain types of cancer, but can also lead to resistance and adverse effects.\n"
     ]
    }
   ],
   "source": [
    "# Reset memory\n",
    "reset_session()\n",
    "\n",
    "# Ask a question from scratch\n",
    "qa = safe_ask_question(\"What fields did Dr. X focus on in his latest publications?\")\n",
    "print(\"🧠 Answer:\\n\", qa)\n",
    "\n",
    "# Translate a response to Arabic\n",
    "print(\"\\n🌍 Arabic Translation:\\n\", translate_text(qa, target_lang=\"ar\"))\n",
    "\n",
    "# Summarize that same answer\n",
    "print(\"\\n📄 Summary:\\n\", summarize_text(qa, strategy=\"insight\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "0cf25572",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing Torch GPU acceleration availability\n",
    "# import torch\n",
    "# print(\"CUDA available:\", torch.cuda.is_available())\n",
    "# print(\"CUDA device:\", torch.cuda.get_device_name(0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1972936",
   "metadata": {},
   "source": [
    "# ✅ Extra Utilities for Enhanced Evaluation, Table Detection, and Creativity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ece96d",
   "metadata": {},
   "source": [
    "#### 🧪 6E. Manual Q&A Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3e29210d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Allows you to manually score LLM answers by relevance, factual accuracy, fluency, and confidence.\n",
    "def evaluate_qa_manual(question: str, answer: str, reference: str = \"\") -> Dict[str, float]:\n",
    "    print(f\"Q: {question}\\nA: {answer}\\nReference: {reference}\\n\")\n",
    "    relevance = float(input(\"Relevance (0-1): \"))\n",
    "    factual = float(input(\"Factual Accuracy (0-1): \"))\n",
    "    fluency = float(input(\"Fluency (0-1): \"))\n",
    "    confidence = float(input(\"Confidence (0-1): \"))\n",
    "    return {\n",
    "        \"Relevance\": relevance,\n",
    "        \"Factual Accuracy\": factual,\n",
    "        \"Fluency\": fluency,\n",
    "        \"Confidence\": confidence\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4fcc162",
   "metadata": {},
   "source": [
    "### 🧩 6F. Table Detection in Text (Used during chunking)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5e2f82c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_table_like(text: str) -> bool:\n",
    "    \"\"\"Heuristic: if lots of tabs or linebreaks, likely a table.\"\"\"\n",
    "    return text.count('\\t') > 5 or text.count('\\n') > 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d54cbef0",
   "metadata": {},
   "source": [
    "### 🔎 6G. Flag Chunk Type (text vs table) during chunking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6efa37ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this to tag chunks\n",
    "\n",
    "def chunk_documents(docs: List[Dict[str, Union[str, int]]]) -> List[Dict[str, Any]]:\n",
    "    all_chunks = []\n",
    "    for doc in docs:\n",
    "        file_name = doc[\"source\"]\n",
    "        page = doc[\"page\"]\n",
    "        chunk_type = \"table\" if detect_table_like(doc[\"text\"]) else \"text\"\n",
    "        split_chunks = chunk_text(doc[\"text\"])\n",
    "        for i, chunk in enumerate(split_chunks):\n",
    "            all_chunks.append({\n",
    "                \"source\": file_name,\n",
    "                \"page\": page,\n",
    "                \"chunk_number\": i + 1,\n",
    "                \"type\": chunk_type,\n",
    "                \"text\": chunk\n",
    "            })\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b1a2bb",
   "metadata": {},
   "source": [
    "#### 🎯 6H. Identify Data-Heavy Questions (e.g., referencing tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "6c177771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_data_query(query: str) -> bool:\n",
    "    keywords = [\"table\", \"dataset\", \"values\", \"results\", \"columns\", \"rows\"]\n",
    "    return any(kw in query.lower() for kw in keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d372520",
   "metadata": {},
   "source": [
    "#### 🧠 6I. Sentence-Based Chunking (Alternative to token-based)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "064bfe31",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GHOST2OM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_by_sentence(text: str, max_tokens=500):\n",
    "    from nltk.tokenize import sent_tokenize\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    for sent in sents:\n",
    "        temp = current_chunk + \" \" + sent\n",
    "        if len(ENCODER.encode(temp)) <= max_tokens:\n",
    "            current_chunk = temp\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9753d2e5",
   "metadata": {},
   "source": [
    "#### ✨ 6J. Post-Translation Grammar Refinement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "2723c23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improves fluency of translated text using LLaMA\n",
    "\n",
    "def refine_translation(text: str, lang: str = \"en\") -> str:\n",
    "    prompt = f\"\"\"\n",
    "You are a fluent editor. Improve the grammar and flow of this translated {lang.upper()} text:\n",
    "\n",
    "{text}\n",
    "\n",
    "Improved Version:\"\"\"\n",
    "    output = llm(prompt, max_tokens=300, stop=[\"\\n\\n\"], echo=False)\n",
    "    return output['choices'][0]['text'].strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe419155",
   "metadata": {},
   "source": [
    "#### 🧠 6K. Display Retrieval Metadata for Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ace6f6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_retrieved_chunks(results):\n",
    "    print(\"\\n📄 Top Retrieved Chunks:\")\n",
    "    for doc, meta in zip(results[\"documents\"][0], results[\"metadatas\"][0]):\n",
    "        print(f\"→ From {meta['source']} (Page {meta['page']}, Chunk {meta['chunk_number']})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650b7266",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GHOST2OM\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# ✅ Enhanced ask_question function with debug logging and table-aware logic\n",
    "\n",
    "def ask_question(query: str, k: int = 5, verbose: bool = False) -> str:\n",
    "    global previous_qas\n",
    "\n",
    "    # 1️⃣ Embed the query\n",
    "    query_embedding = embedding_fn.embed_query(query)\n",
    "\n",
    "    # 2️⃣ Retrieve top-k documents\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=k,\n",
    "        include=[\"documents\", \"metadatas\"]\n",
    "    )\n",
    "\n",
    "    if verbose:\n",
    "        display_retrieved_chunks(results)\n",
    "\n",
    "    # 3️⃣ Prepare prompt with context\n",
    "    context_blocks = []\n",
    "    for i in range(len(results[\"documents\"][0])):\n",
    "        meta = results[\"metadatas\"][0][i]\n",
    "        doc = results[\"documents\"][0][i]\n",
    "        block = f\"Source: {meta['source']} (Page: {meta['page']}, Chunk: {meta['chunk_number']})\\n{doc}\"\n",
    "        context_blocks.append(block)\n",
    "\n",
    "    context_text = \"\\n\\n\".join(context_blocks)\n",
    "    history_text = \"\\n\".join([f\"Q: {q}\\nA: {a}\" for q, a in previous_qas])\n",
    "    prompt = f\"\"\"\n",
    "# You are an intelligent assistant trained to answer questions using only the context provided.\n",
    "You are a helpful AI researcher. Answer the user's question using ONLY the context below.\n",
    "\n",
    "{history_text}\n",
    "\n",
    "Context:\n",
    "{context_text}\n",
    "\n",
    "Q: {query}\n",
    "A:\"\"\".strip()\n",
    "\n",
    "    # 4️⃣ Generate Answer\n",
    "    start_time = time.time()\n",
    "    output = llm(prompt, max_tokens=300, stop=[\"Q:\", \"User:\"], echo=False)\n",
    "    response = output['choices'][0]['text'].strip()\n",
    "    duration = time.time() - start_time\n",
    "    log_tokens(prompt, duration, label=\"LLM Generation\")\n",
    "\n",
    "    if verbose:\n",
    "        print(\"\\n🧠 Generated Answer:\\n\", response)\n",
    "\n",
    "    # 5️⃣ Update memory\n",
    "    previous_qas.append((query, response))\n",
    "\n",
    "    return response\n",
    "\n",
    "\n",
    "# Swap in sentence-based chunking instead of token-based\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def chunk_text(text: str, max_tokens: int = 500, overlap: int = 50) -> List[str]:\n",
    "    \"\"\"Chunk text based on sentences while preserving max token limits.\"\"\"\n",
    "    tokens = ENCODER.encode(text)\n",
    "    sents = sent_tokenize(text)\n",
    "    chunks, current_chunk = [], \"\"\n",
    "    for sent in sents:\n",
    "        temp = current_chunk + \" \" + sent\n",
    "        if len(ENCODER.encode(temp)) <= max_tokens:\n",
    "            current_chunk = temp\n",
    "        else:\n",
    "            chunks.append(current_chunk.strip())\n",
    "            current_chunk = sent\n",
    "    if current_chunk:\n",
    "        chunks.append(current_chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "05be401b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Top Retrieved Chunks:\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 3, Chunk 3)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 3, Chunk 3)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 3, Chunk 3)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 3, Chunk 3)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 3, Chunk 3)\n",
      "🔢 LLM Generation: 1972 tokens in 0.91s → 2167.66 tokens/sec\n",
      "\n",
      "🧠 Generated Answer:\n",
      " Table 1 mentions Cancer stem cells (CSCs) in the 3rd row.\n",
      "Answer: Table 1 mentions Cancer stem cells (CSCs) in the 3rd row.\n"
     ]
    }
   ],
   "source": [
    "response = ask_question(\"What tables mention  Cancer stem cells (CSCs)?\", verbose=True)\n",
    "print(\"Answer:\", response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7cb68b32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reload_llm():\n",
    "    global llm\n",
    "    import gc\n",
    "    del llm\n",
    "    gc.collect()\n",
    "    torch.cuda.empty_cache()\n",
    "    llm = Llama(\n",
    "        model_path=\"models/llama3-8B.gguf\",\n",
    "        n_ctx=3008,\n",
    "        n_threads=16,\n",
    "        n_gpu_layers=GPU_LAYERS,\n",
    "        use_mlock=True,\n",
    "        verbose=False\n",
    "    )\n",
    "    print(\"🔄 LLaMA reloaded and memory cleared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af8fb37b",
   "metadata": {},
   "source": [
    "#### safely restart LLaMA inside code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a863fe1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_init_from_model: n_ctx_per_seq (3008) < n_ctx_train (1048576) -- the full capacity of the model will not be utilized\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 LLaMA reloaded and memory cleared.\n",
      "🧠 Conversation memory cleared.\n"
     ]
    }
   ],
   "source": [
    "reload_llm()\n",
    "# clear_embedding_cache()\n",
    "reset_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "2b7c3231",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "📄 Top Retrieved Chunks:\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 1, Chunk 1)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 1, Chunk 1)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 1, Chunk 1)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 1, Chunk 1)\n",
      "→ From new-approaches-and-procedures-for-cancer-treatment.pdf (Page 1, Chunk 1)\n",
      "🔢 LLM Generation: 2682 tokens in 1.70s → 1574.51 tokens/sec\n",
      "\n",
      "🧠 Generated Answer:\n",
      " Dr. X published an article titled \"New approaches and procedures for cancer treatment: Current perspectives\" in the journal SAGE Open Medicine. The article discusses new developments in cancer treatment and the use of combinatorial strategies involving targeted therapies and traditional chemotherapeutics.\n",
      "Answer: Dr. X published an article titled \"New approaches and procedures for cancer treatment: Current perspectives\" in the journal SAGE Open Medicine. The article discusses new developments in cancer treatment and the use of combinatorial strategies involving targeted therapies and traditional chemotherapeutics.\n",
      "🔢 Safe RAG: 2720 tokens in 5.00s → 543.46 tokens/sec\n",
      "🧠 Answer:\n",
      " Dr. X's latest publication focused on new approaches and procedures for cancer treatment. Specifically, he discussed the use of combinatorial strategies involving targeted therapies and traditional chemotherapeutics.\n",
      "\n",
      "Context:\n",
      "Source: new-approaches-and-procedures-for-cancer-treatment.pdf (Page 6, Chunk 2)\n",
      " and seven studies have been completed.\n",
      "Current clinical trials\n",
      "In recent years, analysis of cancer medication has taken out-\n",
      "standing steps toward more practical, precise, and fewer \n",
      "invasive cancer treatments in the research of clinical trials \n",
      "(Figure 1).\n",
      "Currently, the most frequent entries focusing on cancer \n",
      "therapies in the database of clinical trials (www.clinicalTri-\n",
      "als.gov) include the terminologies stem cell, targeted ther-\n",
      "apy, immunotherapy, and gene therapy because they are \n",
      "very promising and effective. Table 3 summarizes the poten-\n",
      "tial advantages and disadvantages of the new treatment \n",
      "approaches.\n",
      "Table 4 summarizes the approaches to advanced cancer \n",
      "therapies and their respective delivery systems with examples.\n",
      "Conclusion\n",
      "Current methods in oncology focus on the development of \n",
      "safe and efficient cancer nanomedicines. Targeted medical \n",
      "care helped rising the biodistribution of recent or already \n",
      "tested chemotherapeutical agents around the specific tissue \n",
      "to be treated; different methods, such as sequence medical \n",
      "care, siRNAs delivery, therapy, and inhibitor molecules, sup-\n",
      "ply new\n",
      "🔢 Translation: 298 tokens in 0.09s → 3250.47 tokens/sec\n",
      "\n",
      "🌍 Arabic Translation:\n",
      " \n",
      "🔢 Summarization: 303 tokens in 2.34s → 129.45 tokens/sec\n",
      "\n",
      "📄 Summary:\n",
      " - Dr. X's publication discussed the use of combinatorial strategies involving targeted therapies and traditional chemotherapeutics. \n",
      "- Seven studies have been completed, and current clinical trials focus on stem cell, targeted therapy, immunotherapy, and gene therapy. \n",
      "- Table 3 summarized the potential advantages and disadvantages of new treatment approaches. \n",
      "- Table 4 summarized the approaches to advanced cancer therapies and their respective delivery systems with examples. \n",
      "- Current methods in oncology focus on the development of safe and efficient cancer nanomedicines. \n",
      "- Targeted medical care helps increase the biodistribution of recent or already tested chemotherapeutical agents around the specific tissue to be treated. \n",
      "- Different methods, such as sequence medical care, siRNAs delivery, therapy, and inhibitor molecules, provide new opportunities for cancer treatment.\n"
     ]
    }
   ],
   "source": [
    "response = ask_question(\"What Dr. X published??\", verbose=True)\n",
    "print(\"Answer:\", response)\n",
    "\n",
    "# Ask a question from scratch\n",
    "qa = safe_ask_question(\"What fields did Dr. X focus on in his latest publications?\")\n",
    "print(\"🧠 Answer:\\n\", qa)\n",
    "\n",
    "# Translate a response to Arabic\n",
    "print(\"\\n🌍 Arabic Translation:\\n\", translate_text(qa, target_lang=\"ar\"))\n",
    "\n",
    "# Summarize that same answer\n",
    "print(\"\\n📄 Summary:\\n\", summarize_text(qa, strategy=\"insight\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4e09208",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "defae231",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36265102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "356fe3e0",
   "metadata": {},
   "source": [
    "### Trying to reset the Cashe from GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "62f3fb2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # killing llm\n",
    "# import gc\n",
    "# del llm\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "632535d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import gc\n",
    "\n",
    "# def clear_embedding_cache():\n",
    "#     gc.collect()\n",
    "#     del llm\n",
    "#     torch.cuda.empty_cache()\n",
    "#     print(\"PyTorch embedding GPU cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5364920e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "f590fe71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# vec = index_chunks_in_vector_db.embed_query(\"Hello world\")\n",
    "# print(f\"Vector dimension: {len(vec)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e55491fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "\n",
    "# chroma_client = chromadb.Client()\n",
    "# chroma_client.delete_collection(\"my_collection\")  # delete if it already exists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06f06ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
